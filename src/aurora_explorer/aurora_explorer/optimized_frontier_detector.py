#!/usr/bin/env python3
"""
ğŸ” ä¼˜åŒ–çš„å‰æ²¿æ£€æµ‹å™¨
å®ç°è‡ªé€‚åº”DBSCANå‚æ•°è°ƒä¼˜ã€æˆ¿é—´æ„ŸçŸ¥æ¢ç´¢å’Œå‰æ²¿è´¨é‡è¯„ä¼°
Author: Acamana-Bot Development Team
Date: 2025-01-15
"""

import numpy as np
import math
import cv2
from typing import List, Tuple, Dict, Optional
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from scipy import ndimage
from dataclasses import dataclass


@dataclass
class OptimizedFrontierPoint:
    """ä¼˜åŒ–çš„å‰æ²¿ç‚¹æ•°æ®ç»“æ„"""
    x: float                    # ä¸–ç•Œåæ ‡X
    y: float                    # ä¸–ç•Œåæ ‡Y
    size: int                   # å‰æ²¿ç‚¹æ•°é‡
    door_weight: float = 1.0    # é—¨æ´æƒé‡
    quality_score: float = 0.0  # è´¨é‡è¯„åˆ†
    accessibility_score: float = 0.0  # å¯è¾¾æ€§è¯„åˆ†
    room_priority: float = 0.0  # æˆ¿é—´ä¼˜å…ˆçº§
    exploration_value: float = 0.0  # æ¢ç´¢ä»·å€¼


class OptimizedFrontierDetector:
    """ä¼˜åŒ–çš„å‰æ²¿æ£€æµ‹å™¨"""
    
    def __init__(self, map_resolution: float = 0.05, robot_radius: float = 0.35, min_frontier_size: int = 3):
        self.map_resolution = map_resolution
        self.robot_radius = robot_radius

        # ğŸ¯ è‡ªé€‚åº”å‚æ•°
        self.base_eps = 5.0              # åŸºç¡€DBSCAN epså‚æ•°
        self.base_min_samples = 5        # åŸºç¡€æœ€å°æ ·æœ¬æ•°
        self.min_frontier_size = min_frontier_size  # æœ€å°å‰æ²¿å¤§å°(å¯é…ç½®)

        # ğŸ” å¤šå°ºåº¦æ£€æµ‹å‚æ•°
        self.scale_levels = [0.5, 1.0, 1.5]  # å¤šå°ºåº¦æ£€æµ‹çº§åˆ«
        self.adaptive_threshold_enabled = True  # å¯ç”¨è‡ªé€‚åº”é˜ˆå€¼
        self.dynamic_eps_range = (3.0, 8.0)    # åŠ¨æ€epsèŒƒå›´
        
        # ğŸ  æˆ¿é—´æ„ŸçŸ¥å‚æ•°
        self.room_boundary_threshold = 0.1   # æˆ¿é—´è¾¹ç•Œé˜ˆå€¼
        self.door_width_min = 0.6           # æœ€å°é—¨å®½
        self.door_width_max = 1.5           # æœ€å¤§é—¨å®½
        
        # ğŸ“Š è´¨é‡è¯„ä¼°æƒé‡
        self.weights = {
            'size': 0.3,           # å‰æ²¿å¤§å°æƒé‡
            'accessibility': 0.3,   # å¯è¾¾æ€§æƒé‡
            'room_priority': 0.2,   # æˆ¿é—´ä¼˜å…ˆçº§æƒé‡
            'door_bonus': 0.2      # é—¨æ´åŠ æˆæƒé‡
        }
        
        # ğŸ” æ€§èƒ½ç»Ÿè®¡
        self.detection_stats = {
            'total_detections': 0,
            'adaptive_improvements': 0,
            'quality_improvements': 0
        }

    def detect_optimized_frontiers(self, map_data, room_info: Optional[Dict] = None) -> List[OptimizedFrontierPoint]:
        """
        æ£€æµ‹ä¼˜åŒ–çš„å‰æ²¿ç‚¹
        
        Args:
            map_data: åœ°å›¾æ•°æ®
            room_info: æˆ¿é—´ä¿¡æ¯å­—å…¸
            
        Returns:
            ä¼˜åŒ–çš„å‰æ²¿ç‚¹åˆ—è¡¨
        """
        grid = np.array(map_data.data).reshape((map_data.info.height, map_data.info.width))
        
        # ğŸ” Step 1: å¤šå°ºåº¦å‰æ²¿åƒç´ æ£€æµ‹
        frontier_pixels = self._detect_multiscale_frontier_pixels(grid)
        if len(frontier_pixels) == 0:
            return []
        
        # ğŸ¯ Step 2: è‡ªé€‚åº”DBSCANèšç±»
        frontier_clusters = self._adaptive_clustering(frontier_pixels, grid)
        if len(frontier_clusters) == 0:
            return []
        
        # ğŸ“Š Step 3: åŠ¨æ€é˜ˆå€¼è°ƒæ•´å’Œå‰æ²¿è´¨é‡è¯„ä¼°
        if self.adaptive_threshold_enabled:
            self._adjust_dynamic_thresholds(frontier_clusters, grid)

        quality_frontiers = self._evaluate_frontier_quality(frontier_clusters, grid, map_data)
        
        # ğŸ  Step 4: æˆ¿é—´æ„ŸçŸ¥ä¼˜å…ˆçº§è®¡ç®—
        if room_info:
            self._calculate_room_priorities(quality_frontiers, room_info, map_data)
        
        # ğŸ¯ Step 5: ç»¼åˆè¯„åˆ†å’Œæ’åº
        final_frontiers = self._calculate_final_scores(quality_frontiers)
        
        # ğŸ“ˆ æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.detection_stats['total_detections'] += 1
        
        return final_frontiers

    def _detect_multiscale_frontier_pixels(self, grid: np.ndarray) -> List[Tuple[int, int]]:
        """å¤šå°ºåº¦å‰æ²¿åƒç´ æ£€æµ‹"""
        all_frontier_pixels = set()

        for scale in self.scale_levels:
            # ğŸ” åœ¨ä¸åŒå°ºåº¦ä¸‹æ£€æµ‹å‰æ²¿ç‚¹
            scale_frontiers = self._detect_frontier_pixels_at_scale(grid, scale)
            all_frontier_pixels.update(scale_frontiers)

        # ğŸ¯ å»é‡å¹¶è½¬æ¢ä¸ºåˆ—è¡¨
        return list(all_frontier_pixels)

    def _detect_frontier_pixels_at_scale(self, grid: np.ndarray, scale: float) -> List[Tuple[int, int]]:
        """åœ¨æŒ‡å®šå°ºåº¦ä¸‹æ£€æµ‹å‰æ²¿åƒç´  - ä¼˜åŒ–ç‰ˆæœ¬"""
        # ğŸš€ ä½¿ç”¨å‘é‡åŒ–æ“ä½œå¤§å¹…æå‡æ€§èƒ½
        height, width = grid.shape

        # åˆ›å»ºæ©ç ï¼šè‡ªç”±ç©ºé—´
        free_space_mask = (grid == 0)

        # ä½¿ç”¨å½¢æ€å­¦æ“ä½œå¿«é€Ÿæ£€æµ‹è¾¹ç•Œ
        from scipy import ndimage

        # åˆ›å»ºç»“æ„å…ƒç´ 
        detection_radius = max(1, int(scale * 2))
        struct_elem = np.ones((2*detection_radius+1, 2*detection_radius+1))

        # æ£€æµ‹æœªçŸ¥åŒºåŸŸè¾¹ç•Œ
        unknown_mask = (grid == -1)
        unknown_dilated = ndimage.binary_dilation(unknown_mask, structure=struct_elem)

        # æ£€æµ‹éšœç¢ç‰©è¾¹ç•Œ
        obstacle_mask = (grid == 100)
        obstacle_dilated = ndimage.binary_dilation(obstacle_mask, structure=struct_elem)

        # å‰æ²¿ç‚¹ï¼šè‡ªç”±ç©ºé—´ & é‚»è¿‘æœªçŸ¥åŒºåŸŸ & ä¸é‚»è¿‘éšœç¢ç‰©
        frontier_mask = free_space_mask & unknown_dilated & ~obstacle_dilated

        # è·å–å‰æ²¿ç‚¹åæ ‡
        frontier_coords = np.where(frontier_mask)
        frontier_pixels = list(zip(frontier_coords[1], frontier_coords[0]))  # (x, y)æ ¼å¼

        # ğŸ¯ é™åˆ¶å‰æ²¿ç‚¹æ•°é‡ä»¥æé«˜æ€§èƒ½
        if len(frontier_pixels) > 1000:
            # ä½¿ç”¨æ­¥é•¿é‡‡æ ·å‡å°‘å‰æ²¿ç‚¹æ•°é‡
            step = len(frontier_pixels) // 1000
            frontier_pixels = frontier_pixels[::step]

        return frontier_pixels

    def _detect_frontier_pixels(self, grid: np.ndarray) -> List[Tuple[int, int]]:
        """
        æ£€æµ‹å‰æ²¿åƒç´ ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–æ£€æµ‹é€»è¾‘
        """
        # ğŸš€ å‘é‡åŒ–å‰æ²¿æ£€æµ‹ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        h, w = grid.shape

        # åˆ›å»ºæ©ç ï¼šè‡ªç”±ç©ºé—´ (å€¼ä¸º0)
        free_mask = (grid == 0)

        # åˆ›å»ºæœªçŸ¥åŒºåŸŸæ©ç  (å€¼ä¸º-1)
        unknown_mask = (grid == -1)

        # ä½¿ç”¨å·ç§¯æ ¸æ£€æµ‹å‰æ²¿ç‚¹

        # 4-è¿é€šé‚»åŸŸæ ¸
        kernel = np.array([[0, 1, 0],
                          [1, 0, 1],
                          [0, 1, 0]], dtype=np.float32)

        # è®¡ç®—æ¯ä¸ªè‡ªç”±ç©ºé—´åƒç´ å‘¨å›´çš„æœªçŸ¥åŒºåŸŸæ•°é‡
        unknown_neighbors = ndimage.convolve(unknown_mask.astype(np.float32), kernel, mode='constant', cval=0)

        # å‰æ²¿æ¡ä»¶ï¼šè‡ªç”±ç©ºé—´ä¸”è‡³å°‘æœ‰2ä¸ªæœªçŸ¥é‚»å±…
        frontier_mask = free_mask & (unknown_neighbors >= 2)

        # æ·»åŠ è¾¹ç•Œç¼“å†²ï¼ˆé¿å…è¾¹ç•Œåƒç´ ï¼‰
        frontier_mask[0:2, :] = False
        frontier_mask[-2:, :] = False
        frontier_mask[:, 0:2] = False
        frontier_mask[:, -2:] = False

        # è·å–å‰æ²¿åƒç´ åæ ‡
        frontier_coords = np.where(frontier_mask)
        frontier_pixels = [(int(c), int(r)) for r, c in zip(frontier_coords[0], frontier_coords[1])]

        return frontier_pixels

    def _adaptive_clustering(self, frontier_pixels: List[Tuple[int, int]], grid: np.ndarray) -> List[List[Tuple[int, int]]]:
        """
        è‡ªé€‚åº”DBSCANèšç±»
        """
        if len(frontier_pixels) < self.base_min_samples:
            return []
        
        points = np.array(frontier_pixels)
        
        # ğŸ¯ å¤šç»´åº¦ç¯å¢ƒåˆ†æ
        map_density = self._calculate_map_density(grid)
        map_scale = self._estimate_map_scale(grid)
        frontier_distribution = self._analyze_frontier_distribution(points)
        map_complexity = self._calculate_map_complexity(grid)

        # ğŸ”„ åŠ¨æ€å‚æ•°è®¡ç®—
        eps, min_samples = self._calculate_dynamic_parameters(
            map_density, map_scale, frontier_distribution, map_complexity, len(points)
        )
        
        # ğŸ¯ å¤šç­–ç•¥èšç±»å°è¯•
        best_clusters = self._multi_strategy_clustering(points, eps, min_samples)

        return best_clusters

    def _analyze_frontier_distribution(self, points: np.ndarray) -> Dict:
        """åˆ†æå‰æ²¿ç‚¹åˆ†å¸ƒç‰¹å¾ - ä¼˜åŒ–ç‰ˆæœ¬"""
        if len(points) < 2:
            return {'spread': 0.0, 'density': 0.0, 'uniformity': 0.0}

        # ğŸ” è®¡ç®—åˆ†å¸ƒèŒƒå›´
        x_range = np.max(points[:, 0]) - np.min(points[:, 0])
        y_range = np.max(points[:, 1]) - np.min(points[:, 1])
        spread = math.sqrt(x_range**2 + y_range**2)

        # ğŸ” è®¡ç®—å¯†åº¦
        area = max(x_range * y_range, 1)
        density = len(points) / area

        # ğŸš€ ä¼˜åŒ–å‡åŒ€æ€§è®¡ç®—ï¼šä½¿ç”¨é‡‡æ ·è€Œä¸æ˜¯å…¨è·ç¦»è®¡ç®—
        if len(points) > 100:
            # å¯¹äºå¤§é‡ç‚¹ï¼Œéšæœºé‡‡æ ·è®¡ç®—å‡åŒ€æ€§
            sample_size = min(100, len(points))
            indices = np.random.choice(len(points), sample_size, replace=False)
            sample_points = points[indices]
        else:
            sample_points = points

        # ä½¿ç”¨ç®€åŒ–çš„å‡åŒ€æ€§åº¦é‡ï¼šæ ‡å‡†å·®/å‡å€¼
        center = np.mean(sample_points, axis=0)
        distances_to_center = np.linalg.norm(sample_points - center, axis=1)
        uniformity = 1.0 / (1.0 + np.std(distances_to_center) / (np.mean(distances_to_center) + 1e-6))

        return {
            'spread': spread,
            'density': density,
            'uniformity': uniformity
        }

    def _calculate_map_complexity(self, grid: np.ndarray) -> float:
        """è®¡ç®—åœ°å›¾å¤æ‚åº¦ - ä¼˜åŒ–ç‰ˆæœ¬"""
        # ğŸš€ ä½¿ç”¨ç®€åŒ–çš„å¤æ‚åº¦è®¡ç®—ï¼Œé¿å…æ˜‚è´µçš„è¾¹ç¼˜æ£€æµ‹
        height, width = grid.shape

        # å¯¹å¤§åœ°å›¾è¿›è¡Œä¸‹é‡‡æ ·ä»¥æé«˜æ€§èƒ½
        if height > 200 or width > 200:
            # ä¸‹é‡‡æ ·åˆ°åˆç†å°ºå¯¸
            scale_factor = min(200 / height, 200 / width)
            new_height = int(height * scale_factor)
            new_width = int(width * scale_factor)
            grid_small = cv2.resize(grid.astype(np.uint8), (new_width, new_height), interpolation=cv2.INTER_NEAREST)
        else:
            grid_small = grid

        # ä½¿ç”¨ç®€å•çš„æ¢¯åº¦è®¡ç®—ä»£æ›¿Cannyè¾¹ç¼˜æ£€æµ‹
        obstacle_mask = (grid_small == 100)

        # è®¡ç®—æ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆè¾¹ç¼˜æ£€æµ‹ï¼‰
        grad_x = np.abs(np.diff(obstacle_mask.astype(float), axis=1))
        grad_y = np.abs(np.diff(obstacle_mask.astype(float), axis=0))

        edge_density = (np.sum(grad_x) + np.sum(grad_y)) / (grid_small.size)

        return min(edge_density * 5, 1.0)  # å½’ä¸€åŒ–åˆ°0-1

    def _calculate_dynamic_parameters(self, map_density: float, map_scale: float,
                                    frontier_dist: Dict, map_complexity: float,
                                    point_count: int) -> Tuple[float, int]:
        """åŠ¨æ€è®¡ç®—DBSCANå‚æ•°"""
        # ğŸ¯ åŸºç¡€å‚æ•°è°ƒæ•´
        base_eps = self.base_eps
        base_min_samples = self.base_min_samples

        # ğŸ”„ æ ¹æ®åœ°å›¾å¯†åº¦è°ƒæ•´
        if map_density > 0.4:  # é«˜å¯†åº¦ç¯å¢ƒ
            eps_factor = 0.6
            min_samples_factor = 0.8
        elif map_density < 0.15:  # ä½å¯†åº¦ç¯å¢ƒ
            eps_factor = 1.4
            min_samples_factor = 0.6
        else:  # ä¸­ç­‰å¯†åº¦ç¯å¢ƒ
            eps_factor = 1.0
            min_samples_factor = 1.0

        # ğŸ”„ æ ¹æ®å‰æ²¿ç‚¹åˆ†å¸ƒè°ƒæ•´
        if frontier_dist['spread'] > 100:  # åˆ†å¸ƒå¾ˆå¹¿
            eps_factor *= 1.2
        elif frontier_dist['spread'] < 20:  # åˆ†å¸ƒå¾ˆå¯†
            eps_factor *= 0.8

        # ğŸ”„ æ ¹æ®åœ°å›¾å¤æ‚åº¦è°ƒæ•´
        if map_complexity > 0.5:  # å¤æ‚ç¯å¢ƒ
            min_samples_factor *= 1.2

        # ğŸ”„ æ ¹æ®ç‚¹æ•°é‡è°ƒæ•´
        if point_count > 200:  # å¤§é‡å‰æ²¿ç‚¹
            min_samples_factor *= 1.1
        elif point_count < 50:  # å°‘é‡å‰æ²¿ç‚¹
            min_samples_factor *= 0.8

        # ğŸ¯ è®¡ç®—æœ€ç»ˆå‚æ•°
        final_eps = base_eps * eps_factor * map_scale
        final_min_samples = max(2, int(base_min_samples * min_samples_factor))

        # ğŸ”’ é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        final_eps = max(self.dynamic_eps_range[0], min(self.dynamic_eps_range[1], final_eps))
        final_min_samples = max(2, min(10, final_min_samples))

        return final_eps, final_min_samples

    def _multi_strategy_clustering(self, points: np.ndarray, eps: float, min_samples: int) -> List[List[Tuple[int, int]]]:
        """å¤šç­–ç•¥èšç±»å°è¯•"""
        strategies = [
            (eps, min_samples),                    # ä¸»ç­–ç•¥
            (eps * 0.8, min_samples),             # æ›´ç´§å¯†èšç±»
            (eps * 1.2, max(2, min_samples - 1)), # æ›´æ¾æ•£èšç±»
            (self.base_eps, self.base_min_samples) # å¤‡é€‰ç­–ç•¥
        ]

        best_clusters = []
        best_quality = -1.0

        for strategy_eps, strategy_min_samples in strategies:
            try:
                # ğŸš€ å¹¶è¡ŒDBSCANèšç±»
                clustering = DBSCAN(eps=strategy_eps, min_samples=strategy_min_samples, n_jobs=-1).fit(points)
                labels = clustering.labels_
                clusters = self._extract_clusters(points, labels)

                if len(clusters) > 0:
                    # ğŸ¯ è¯„ä¼°èšç±»è´¨é‡
                    quality = self._evaluate_clustering_quality(points, labels)

                    if quality > best_quality:
                        best_quality = quality
                        best_clusters = clusters

                    # å¦‚æœè´¨é‡è¶³å¤Ÿå¥½ï¼Œæå‰è¿”å›
                    if quality > 0.7:
                        break

            except Exception:
                continue

        return best_clusters

    def _adjust_dynamic_thresholds(self, frontier_clusters: List[List[Tuple[int, int]]], grid: np.ndarray) -> None:
        """åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼"""
        if not frontier_clusters:
            return

        # ğŸ” åˆ†æå½“å‰æ£€æµ‹ç»“æœ
        cluster_sizes = [len(cluster) for cluster in frontier_clusters]
        avg_cluster_size = np.mean(cluster_sizes)
        cluster_count = len(frontier_clusters)

        # ğŸ¯ æ ¹æ®æ£€æµ‹ç»“æœè°ƒæ•´å‚æ•°
        if cluster_count > 20:  # æ£€æµ‹åˆ°å¤ªå¤šå°èšç±»
            self.min_frontier_size = min(self.min_frontier_size + 1, 8)
            self.base_eps = min(self.base_eps * 1.1, self.dynamic_eps_range[1])
        elif cluster_count < 3 and avg_cluster_size > 15:  # æ£€æµ‹åˆ°å¤ªå°‘å¤§èšç±»
            self.min_frontier_size = max(self.min_frontier_size - 1, 2)
            self.base_eps = max(self.base_eps * 0.9, self.dynamic_eps_range[0])

        # ğŸ”„ æ ¹æ®åœ°å›¾æ¢ç´¢è¿›åº¦è°ƒæ•´
        map_exploration_ratio = self._calculate_exploration_ratio(grid)
        if map_exploration_ratio > 0.8:  # åæœŸæ¢ç´¢ï¼Œéœ€è¦æ›´ç²¾ç»†çš„æ£€æµ‹
            self.min_frontier_size = max(2, self.min_frontier_size - 1)
        elif map_exploration_ratio < 0.3:  # æ—©æœŸæ¢ç´¢ï¼Œå¯ä»¥æ›´ç²—ç³™
            self.min_frontier_size = min(6, self.min_frontier_size + 1)

    def _calculate_exploration_ratio(self, grid: np.ndarray) -> float:
        """è®¡ç®—åœ°å›¾æ¢ç´¢æ¯”ä¾‹"""
        total_cells = grid.size
        unknown_cells = np.sum(grid == -1)
        return 1.0 - (unknown_cells / max(total_cells, 1))

    def _calculate_map_density(self, grid: np.ndarray) -> float:
        """è®¡ç®—åœ°å›¾å¯†åº¦"""
        total_cells = grid.size
        free_cells = np.sum(grid == 0)
        occupied_cells = np.sum(grid > 0)
        
        if total_cells == 0:
            return 0.0
        
        return (free_cells + occupied_cells) / total_cells

    def _estimate_map_scale(self, grid: np.ndarray) -> float:
        """ä¼°è®¡åœ°å›¾å°ºåº¦å› å­"""
        # åŸºäºåœ°å›¾å¤§å°çš„ç®€å•å°ºåº¦ä¼°è®¡
        map_area = grid.shape[0] * grid.shape[1]
        
        # å½’ä¸€åŒ–åˆ°0.5-2.0èŒƒå›´
        if map_area < 100000:      # å°åœ°å›¾
            return 0.5
        elif map_area > 1000000:   # å¤§åœ°å›¾
            return 2.0
        else:                      # ä¸­ç­‰åœ°å›¾
            return 1.0

    def _evaluate_clustering_quality(self, points: np.ndarray, labels: np.ndarray) -> float:
        """è¯„ä¼°èšç±»è´¨é‡"""
        try:
            # æ’é™¤å™ªå£°ç‚¹
            mask = labels != -1
            if np.sum(mask) < 2:
                return 0.0
            
            filtered_points = points[mask]
            filtered_labels = labels[mask]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„èšç±»
            unique_labels = len(set(filtered_labels))
            if unique_labels < 2:
                return 0.0
            
            # ä½¿ç”¨è½®å»“ç³»æ•°è¯„ä¼°
            silhouette = silhouette_score(filtered_points, filtered_labels)
            
            # é¢å¤–è€ƒè™‘èšç±»æ•°é‡çš„åˆç†æ€§
            cluster_count_penalty = 1.0
            if unique_labels > 10:  # å¤ªå¤šèšç±»
                cluster_count_penalty = 0.8
            elif unique_labels < 2:  # å¤ªå°‘èšç±»
                cluster_count_penalty = 0.6
            
            return max(0.0, silhouette * cluster_count_penalty)
            
        except Exception:
            return 0.0

    def _extract_clusters(self, points: np.ndarray, labels: np.ndarray) -> List[List[Tuple[int, int]]]:
        """ä»èšç±»æ ‡ç­¾ä¸­æå–èšç±»"""
        clusters = []
        unique_labels = set(labels)
        
        for label in unique_labels:
            if label == -1:  # è·³è¿‡å™ªå£°ç‚¹
                continue
            
            cluster_points = points[labels == label]
            if len(cluster_points) >= self.min_frontier_size:
                cluster_list = [(int(p[0]), int(p[1])) for p in cluster_points]
                clusters.append(cluster_list)
        
        return clusters

    def _evaluate_frontier_quality(self, clusters: List[List[Tuple[int, int]]], 
                                 grid: np.ndarray, map_data) -> List[OptimizedFrontierPoint]:
        """è¯„ä¼°å‰æ²¿è´¨é‡å¹¶åˆ›å»ºä¼˜åŒ–çš„å‰æ²¿ç‚¹"""
        quality_frontiers = []
        
        for cluster in clusters:
            # ğŸ¯ è®¡ç®—å®‰å…¨çš„è´¨å¿ƒç›®æ ‡ç‚¹
            target_x, target_y = self._find_safe_centroid_target(grid, cluster)
            if target_x is None:
                continue
            
            # è½¬æ¢ä¸ºä¸–ç•Œåæ ‡
            world_x = map_data.info.origin.position.x + target_x * map_data.info.resolution
            world_y = map_data.info.origin.position.y + target_y * map_data.info.resolution
            
            # ğŸ” åˆ›å»ºä¼˜åŒ–çš„å‰æ²¿ç‚¹
            frontier = OptimizedFrontierPoint(
                x=world_x,
                y=world_y,
                size=len(cluster)
            )
            
            # ğŸ“Š è¯„ä¼°å„é¡¹è´¨é‡æŒ‡æ ‡
            frontier.door_weight = self._compute_enhanced_door_weight(grid, target_x, target_y, map_data)
            frontier.quality_score = self._compute_frontier_quality_score(cluster, grid, target_x, target_y)
            frontier.accessibility_score = self._compute_accessibility_score(grid, target_x, target_y)
            
            # ğŸ“ˆ åªä¿ç•™è´¨é‡è¶³å¤Ÿé«˜çš„å‰æ²¿ç‚¹ (é™ä½é˜ˆå€¼ä»¥ç¡®ä¿å‰æ²¿ç‚¹å¯è§)
            if frontier.quality_score > 0.1 and frontier.accessibility_score > 0.1:
                quality_frontiers.append(frontier)
                self.detection_stats['quality_improvements'] += 1
        
        return quality_frontiers

    def _find_safe_centroid_target(self, grid: np.ndarray, cluster: List[Tuple[int, int]]) -> Tuple[Optional[int], Optional[int]]:
        """å¯»æ‰¾å®‰å…¨çš„è´¨å¿ƒç›®æ ‡ç‚¹"""
        if not cluster:
            return None, None
        
        # è®¡ç®—è´¨å¿ƒ
        cluster_array = np.array(cluster)
        center_x = np.mean(cluster_array[:, 0])
        center_y = np.mean(cluster_array[:, 1])
        
        # æŒ‰è·ç¦»è´¨å¿ƒçš„è·ç¦»æ’åº
        sorted_points = sorted(cluster, key=lambda p: (p[0] - center_x)**2 + (p[1] - center_y)**2)
        
        # å¯»æ‰¾ç¬¬ä¸€ä¸ªå®‰å…¨çš„ç‚¹
        robot_radius_cells = int(self.robot_radius / self.map_resolution)
        
        for px, py in sorted_points:
            if self._is_safe_target(grid, px, py, robot_radius_cells):
                return px, py
        
        return None, None

    def _is_safe_target(self, grid: np.ndarray, x: int, y: int, radius_cells: int) -> bool:
        """æ£€æŸ¥ç›®æ ‡ç‚¹æ˜¯å¦å®‰å…¨"""
        for dy in range(-radius_cells, radius_cells + 1):
            for dx in range(-radius_cells, radius_cells + 1):
                check_x, check_y = x + dx, y + dy
                if 0 <= check_x < grid.shape[1] and 0 <= check_y < grid.shape[0]:
                    if grid[check_y, check_x] > 0:  # é‡åˆ°éšœç¢ç‰©
                        return False
                # è¶…å‡ºè¾¹ç•Œä¹Ÿè®¤ä¸ºä¸å®‰å…¨
                elif not (0 <= check_x < grid.shape[1] and 0 <= check_y < grid.shape[0]):
                    return False
        return True

    def _compute_enhanced_door_weight(self, grid: np.ndarray, x: int, y: int, map_data) -> float:
        """å¢å¼ºçš„é—¨æ´æƒé‡è®¡ç®—"""
        try:
            # ğŸšª å¤šæ–¹å‘é—¨æ´æ£€æµ‹
            directions = [
                (1, 0),   # æ°´å¹³
                (0, 1),   # å‚ç›´
                (1, 1),   # å¯¹è§’çº¿
                (1, -1)   # åå¯¹è§’çº¿
            ]
            
            max_door_score = 1.0
            
            for dx, dy in directions:
                # æ­£å‘å’Œè´Ÿå‘æ‰«æ
                pos_dist = self._scan_free_space(grid, x, y, dx, dy)
                neg_dist = self._scan_free_space(grid, x, y, -dx, -dy)
                
                total_width = (pos_dist + neg_dist) * self.map_resolution
                
                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé—¨æ´ç‰¹å¾
                if self.door_width_min < total_width < self.door_width_max:
                    # è®¡ç®—é—¨æ´è¯„åˆ†
                    door_score = 1.5 + min(0.5, (total_width - self.door_width_min) / (self.door_width_max - self.door_width_min))
                    max_door_score = max(max_door_score, door_score)
            
            return max_door_score
            
        except Exception:
            return 1.0

    def _scan_free_space(self, grid: np.ndarray, start_x: int, start_y: int, dx: int, dy: int) -> int:
        """æ‰«æè‡ªç”±ç©ºé—´è·ç¦»"""
        distance = 0
        x, y = start_x, start_y
        
        while 0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]:
            if grid[y, x] != 0:  # é‡åˆ°éè‡ªç”±ç©ºé—´
                break
            distance += 1
            x += dx
            y += dy
            
            # é˜²æ­¢æ— é™æ‰«æ
            if distance > 50:
                break
        
        return distance

    def _compute_frontier_quality_score(self, cluster: List[Tuple[int, int]], 
                                      grid: np.ndarray, target_x: int, target_y: int) -> float:
        """è®¡ç®—å‰æ²¿è´¨é‡è¯„åˆ†"""
        try:
            # ğŸ¯ åŸºç¡€è¯„åˆ†å› å­
            size_score = min(1.0, len(cluster) / 20.0)  # å¤§å°è¯„åˆ†
            
            # ğŸ” å‘¨å›´æœªçŸ¥åŒºåŸŸå¯†åº¦
            unknown_density = self._calculate_unknown_density(grid, target_x, target_y, radius=5)
            
            # ğŸ“Š å‰æ²¿å½¢çŠ¶ç´§å‡‘æ€§
            compactness = self._calculate_cluster_compactness(cluster)
            
            # ğŸ¯ ç»¼åˆè´¨é‡è¯„åˆ†
            quality = (size_score * 0.4 + unknown_density * 0.4 + compactness * 0.2)
            
            return min(1.0, max(0.0, quality))
            
        except Exception:
            return 0.5

    def _calculate_unknown_density(self, grid: np.ndarray, x: int, y: int, radius: int = 5) -> float:
        """è®¡ç®—å‘¨å›´æœªçŸ¥åŒºåŸŸå¯†åº¦"""
        unknown_count = 0
        total_count = 0
        
        for dy in range(-radius, radius + 1):
            for dx in range(-radius, radius + 1):
                check_x, check_y = x + dx, y + dy
                if 0 <= check_x < grid.shape[1] and 0 <= check_y < grid.shape[0]:
                    total_count += 1
                    if grid[check_y, check_x] == -1:
                        unknown_count += 1
        
        return unknown_count / total_count if total_count > 0 else 0.0

    def _calculate_cluster_compactness(self, cluster: List[Tuple[int, int]]) -> float:
        """è®¡ç®—èšç±»ç´§å‡‘æ€§"""
        if len(cluster) <= 1:
            return 1.0
        
        cluster_array = np.array(cluster)
        center = np.mean(cluster_array, axis=0)
        
        # è®¡ç®—å¹³å‡è·ç¦»å’Œæ ‡å‡†å·®
        distances = [np.linalg.norm(point - center) for point in cluster_array]
        avg_distance = np.mean(distances)
        std_distance = np.std(distances)
        
        # ç´§å‡‘æ€§ = 1 / (1 + å˜å¼‚ç³»æ•°)
        if avg_distance == 0:
            return 1.0
        
        coefficient_variation = std_distance / avg_distance
        compactness = 1.0 / (1.0 + coefficient_variation)
        
        return compactness

    def _compute_accessibility_score(self, grid: np.ndarray, x: int, y: int) -> float:
        """è®¡ç®—å¯è¾¾æ€§è¯„åˆ†"""
        try:
            # ğŸ›£ï¸ æ£€æŸ¥å¤šä¸ªæ–¹å‘çš„å¯è¾¾æ€§
            directions = [(1, 0), (-1, 0), (0, 1), (0, -1), (1, 1), (1, -1), (-1, 1), (-1, -1)]
            accessible_directions = 0
            
            for dx, dy in directions:
                if self._check_direction_accessibility(grid, x, y, dx, dy, max_distance=10):
                    accessible_directions += 1
            
            # å¯è¾¾æ€§è¯„åˆ†åŸºäºå¯è¾¾æ–¹å‘çš„æ¯”ä¾‹
            accessibility = accessible_directions / len(directions)
            
            # ğŸ¯ é¢å¤–æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è‡ªç”±ç©ºé—´
            free_space_ratio = self._calculate_nearby_free_space(grid, x, y, radius=3)
            
            # ç»¼åˆå¯è¾¾æ€§è¯„åˆ†
            final_score = (accessibility * 0.7 + free_space_ratio * 0.3)
            
            return min(1.0, max(0.0, final_score))
            
        except Exception:
            return 0.5

    def _check_direction_accessibility(self, grid: np.ndarray, start_x: int, start_y: int, 
                                     dx: int, dy: int, max_distance: int = 10) -> bool:
        """æ£€æŸ¥ç‰¹å®šæ–¹å‘çš„å¯è¾¾æ€§"""
        x, y = start_x, start_y
        
        for _ in range(max_distance):
            x += dx
            y += dy
            
            if not (0 <= x < grid.shape[1] and 0 <= y < grid.shape[0]):
                return False
            
            if grid[y, x] > 0:  # é‡åˆ°éšœç¢ç‰©
                return False
            
            if grid[y, x] == 0:  # åˆ°è¾¾è‡ªç”±ç©ºé—´
                return True
        
        return True

    def _calculate_nearby_free_space(self, grid: np.ndarray, x: int, y: int, radius: int = 3) -> float:
        """è®¡ç®—é™„è¿‘è‡ªç”±ç©ºé—´æ¯”ä¾‹"""
        free_count = 0
        total_count = 0
        
        for dy in range(-radius, radius + 1):
            for dx in range(-radius, radius + 1):
                check_x, check_y = x + dx, y + dy
                if 0 <= check_x < grid.shape[1] and 0 <= check_y < grid.shape[0]:
                    total_count += 1
                    if grid[check_y, check_x] == 0:
                        free_count += 1
        
        return free_count / total_count if total_count > 0 else 0.0

    def _calculate_room_priorities(self, frontiers: List[OptimizedFrontierPoint], 
                                 room_info: Dict, map_data) -> None:
        """è®¡ç®—æˆ¿é—´ä¼˜å…ˆçº§"""
        if not room_info or 'room_centroids' not in room_info:
            return
        
        room_centroids = room_info['room_centroids']
        room_stats = room_info.get('room_stats', {})
        
        for frontier in frontiers:
            # ğŸ  æ‰¾åˆ°æœ€è¿‘çš„æˆ¿é—´
            min_distance = float('inf')
            closest_room_id = None
            
            for room_id, (room_x, room_y) in room_centroids.items():
                distance = math.sqrt((frontier.x - room_x)**2 + (frontier.y - room_y)**2)
                if distance < min_distance:
                    min_distance = distance
                    closest_room_id = room_id
            
            # ğŸ“Š è®¡ç®—æˆ¿é—´ä¼˜å…ˆçº§
            if closest_room_id and closest_room_id in room_stats:
                stats = room_stats[closest_room_id]
                
                # åŸºäºæˆ¿é—´é¢ç§¯ã€è¾¹ç•Œå› å­å’ŒæœªçŸ¥æ¯”ä¾‹è®¡ç®—ä¼˜å…ˆçº§
                area_factor = min(1.0, stats.get('area', 0) / 1000.0)
                boundary_factor = stats.get('boundary_factor', 1.0)
                unknown_ratio = stats.get('unknown_ratio', 0.0)
                
                # è·ç¦»æƒ©ç½š
                distance_penalty = 1.0 / (1.0 + min_distance / 5.0)
                
                room_priority = (area_factor * 0.3 + 
                               boundary_factor * 0.3 + 
                               unknown_ratio * 0.2 + 
                               distance_penalty * 0.2)
                
                frontier.room_priority = min(1.0, max(0.0, room_priority))

    def _calculate_final_scores(self, frontiers: List[OptimizedFrontierPoint]) -> List[OptimizedFrontierPoint]:
        """è®¡ç®—æœ€ç»ˆç»¼åˆè¯„åˆ†"""
        for frontier in frontiers:
            # ğŸ¯ ç»¼åˆè¯„åˆ†è®¡ç®—
            size_component = min(1.0, frontier.size / 20.0) * self.weights['size']
            accessibility_component = frontier.accessibility_score * self.weights['accessibility']
            room_component = frontier.room_priority * self.weights['room_priority']
            door_component = (frontier.door_weight - 1.0) * self.weights['door_bonus']
            
            # ğŸ“Š æœ€ç»ˆæ¢ç´¢ä»·å€¼
            frontier.exploration_value = (size_component + 
                                        accessibility_component + 
                                        room_component + 
                                        door_component + 
                                        frontier.quality_score * 0.1)
        
        # æŒ‰æ¢ç´¢ä»·å€¼æ’åº
        frontiers.sort(key=lambda f: f.exploration_value, reverse=True)
        
        return frontiers

    def get_detection_stats(self) -> Dict:
        """è·å–æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        return self.detection_stats.copy()

    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.detection_stats = {
            'total_detections': 0,
            'adaptive_improvements': 0,
            'quality_improvements': 0
        } 